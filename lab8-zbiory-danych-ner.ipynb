{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tworzenie zasobów + wykrywanie encji nazwanych\n",
    "\n",
    "Algorytmy wykorzystywane w problemach przetwarzania języka naturalnego opierają najczęściej swoje działanie o analizę dużych korpusów danych. O ile w zadaniach konkursowych często odpowiednie dane są już przygotowane, o tyle tworząc własne eksperymenty, często musimy sami pozyskać dane i przetransformować do użytecznej postaci.\n",
    "\n",
    "Dzisiejsze laboratoria dotyczyć będą tworzenia korpusów danych, tworzenia reprezentacji CoNNL i wykorzystania jej do zadania wykrywania encji nazwanych.\n",
    "\n",
    "## Automatyczne pozyskiwanie surowych danych tekstowych\n",
    "Dotychczas omawiane metody działały na surowym tekście, który transformowany był do odpowiedniej reprezentacji wektorowej (Bag of words, bag of ngrams, embeddingi). Jak zautomatyzować pozyskiwanie takich surowych danych z internetu?\n",
    "\n",
    "W tej części skupimy się na stworzeniu automatycznego pobieracza danych, który działać będzie w dwóch \"obszarach\":\n",
    "<ol>\n",
    "<li>crawler: moduł odwiedzający kolejne strony internetowy</li>\n",
    "<li>scraper: moduł ekstrahujący treść z konkretnych stron internetowych</li>\n",
    "</ol>\n",
    "\n",
    "Wykorzystajmy do tego dwie biblioteki: \n",
    "\n",
    "**urllib** - do odwiedzania stron\n",
    "\n",
    "**BeautifulSoup** - do parsowania danych (np. w formacie HTML).\n",
    "\n",
    "## Zadanie1 (2pkt): Napisz prosty ekstraktor danych ze stron WWW odwiedzający kilka podstron\n",
    "Ekstraktor ma odwiedzić zadaną stronę internetową, pobrać zawartość wszystkich tekstów wewnątrz paragrafów (wewnątrz tagów P zawartych w pobranym dokumencie HTML), a następnie odwiedzić 5 dowolnych linków z tej strony i z nich analogicznie pobrać zawartość.\n",
    "Łącznie powinniśmy otrzymać dane z 6 adresów internetowch (strona główna + 5 linków ze strony głównej).\n",
    "\n",
    "Do napisania crawlera przydać się mogą następujące funkcje:\n",
    "\n",
    "urllib.request.urlopen() - do pobrania zawartości strony\n",
    "findAll() na obiekcie BeautifulSoup, można ją wykorzystać do przeiterowania po wszystkich tagach danego rodzaju\n",
    "get_text() - Istnieje duża szansa, że wewnątrz tagów P znajdą się również inne tagi HTML, chcielibyśmy oczyścić \n",
    "z nich tekst. Można to zrobić albo z wyrażeniami regularnymi (robiliśmy takie zadanie na pierwszych laboratoriach!), albo użyć właśnie funkcji get_text() z BeautifulSoup\n",
    "\n",
    "Linki do dokumentacji:\n",
    "urllib, pobieranie danych: https://docs.python.org/3/howto/urllib2.html\n",
    "beautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ (przeczytanie QuickStart jest wystarczające do zrobienia tego zadania)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W październiku 2016 r. Gmina Suchy Las złożyła wniosek o dofinansowanie projektu związanego z kompleksową termomodernizacją budynku Zespołu Szkół w Chludowie. Projekt przeszedł pozytywną ocenę strategiczną, formalną i merytoryczną i decyzją Zarządu Województwa Wielkopolskiego otrzymał dofinansowanie w wysokości 85% kosztów kwalifikowanych.\n",
      "W czerwcu 2017 r.  Wójt Gminy podpisał umowę o dofinansowanie.\n",
      "Realizacja inwestycji polegała na zmniejszeniu zapotrzebowania na energię dostarczaną na potrzeby ogrzewania, oświetlenia, ograniczenia strat energii i redukcję zanieczyszczeń powietrza poprzez obniżenie ilości substancji zanieczyszczających, wytwarzanych w procesie energetycznego spalania paliwa. Projekt obejmował rozwiązania zmierzające do poprawy stanu technicznego budynku, warunków cieplnych i zwiększenia energooszczędności oraz poprawy komfortu użytkowników.\n",
      "W zakres prac wchodziło m.in.: docieplenie obiektu, wymiana stolarki okiennej i drzwiowej, wymiana instalacji oświetlenia na energooszczędne, wymiana grzejników i montaż paneli fotowoltaicznych.  _ \n",
      "Całkowita wartość Projektu, to 3.178.275,01 zł., natomiast kwota dofinansowania z Unii Europejskiej, to  2.701.533,75 zł.\n",
      "Projekt współfinansowany przez Unię Europejską z Europejskiego Funduszu Rozwoju Regionalnego w ramach Wielkopolskiego Regionalnego Programu Operacyjnego na lata 2014-2020\n",
      "FUNDUSZE EUROPEJSKIE – LEPSZA JAKOŚĆ ŻYCIA W WIELKOPOLSCE\n",
      "Warsztaty filmowo-plastyczne, których efektem jest film animowany poruszający temat wpływu środowiska na proces uzależnienia jednostki, sposobów minimalizowania negatywnych czynników oraz roli szkoły i środowiska szkolno-wychowawczego w kształtowaniu pozytywnych zachowań.Uczestnicy wzięli udział w rozmowie na powyższy temat, a następnie stworzyli scenariusz z uwzględnieniem kreatywnego wykorzystania przestrzeni i znajdujących się w niej przedmiotów oraz film animowany w technikach animacji poklatkowej (piksilacja – animacja aktora, animacja przedmiotów znalezionych). Fabuła filmu zbudowana jest wokół metafory światła, jakopozytywnego wzorca postaw i zachowań, oraz cienia, jako szkodliwego otoczenia wpływającego na rozwój uzależnień. Szczególny nacisk położony został na kreowanie pozytywnego wizerunku szkoły jako przestrzeni sprzyjającej zdrowemu trybowi życia. Dokonane to zostało poprzez jej animizację, jako żywej, przychylnej i przyjaznej przestrzeni, w której przedmioty, takie jak piłka   sali gimnastycznej czy książki w bibliotece, same wychodzą naprzeciw uczniom, zachęcając ich i wspierając w rozwijaniu pasji i zainteresowań.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Młodzieżowa Rada Gminy Suchy Las \n",
      "entering link http://chludowo.pl\n",
      "W październiku 2016 r. Gmina Suchy Las złożyła wniosek o dofinansowanie projektu związanego z kompleksową termomodernizacją budynku Zespołu Szkół w Chludowie. Projekt przeszedł pozytywną ocenę strategiczną, formalną i merytoryczną i decyzją Zarządu Województwa Wielkopolskiego otrzymał dofinansowanie w wysokości 85% kosztów kwalifikowanych.\n",
      "W czerwcu 2017 r.  Wójt Gminy podpisał umowę o dofinansowanie.\n",
      "Realizacja inwestycji polegała na zmniejszeniu zapotrzebowania na energię dostarczaną na potrzeby ogrzewania, oświetlenia, ograniczenia strat energii i redukcję zanieczyszczeń powietrza poprzez obniżenie ilości substancji zanieczyszczających, wytwarzanych w procesie energetycznego spalania paliwa. Projekt obejmował rozwiązania zmierzające do poprawy stanu technicznego budynku, warunków cieplnych i zwiększenia energooszczędności oraz poprawy komfortu użytkowników.\n",
      "W zakres prac wchodziło m.in.: docieplenie obiektu, wymiana stolarki okiennej i drzwiowej, wymiana instalacji oświetlenia na energooszczędne, wymiana grzejników i montaż paneli fotowoltaicznych.  _ \n",
      "Całkowita wartość Projektu, to 3.178.275,01 zł., natomiast kwota dofinansowania z Unii Europejskiej, to  2.701.533,75 zł.\n",
      "Projekt współfinansowany przez Unię Europejską z Europejskiego Funduszu Rozwoju Regionalnego w ramach Wielkopolskiego Regionalnego Programu Operacyjnego na lata 2014-2020\n",
      "FUNDUSZE EUROPEJSKIE – LEPSZA JAKOŚĆ ŻYCIA W WIELKOPOLSCE\n",
      "Warsztaty filmowo-plastyczne, których efektem jest film animowany poruszający temat wpływu środowiska na proces uzależnienia jednostki, sposobów minimalizowania negatywnych czynników oraz roli szkoły i środowiska szkolno-wychowawczego w kształtowaniu pozytywnych zachowań.Uczestnicy wzięli udział w rozmowie na powyższy temat, a następnie stworzyli scenariusz z uwzględnieniem kreatywnego wykorzystania przestrzeni i znajdujących się w niej przedmiotów oraz film animowany w technikach animacji poklatkowej (piksilacja – animacja aktora, animacja przedmiotów znalezionych). Fabuła filmu zbudowana jest wokół metafory światła, jakopozytywnego wzorca postaw i zachowań, oraz cienia, jako szkodliwego otoczenia wpływającego na rozwój uzależnień. Szczególny nacisk położony został na kreowanie pozytywnego wizerunku szkoły jako przestrzeni sprzyjającej zdrowemu trybowi życia. Dokonane to zostało poprzez jej animizację, jako żywej, przychylnej i przyjaznej przestrzeni, w której przedmioty, takie jak piłka   sali gimnastycznej czy książki w bibliotece, same wychodzą naprzeciw uczniom, zachęcając ich i wspierając w rozwijaniu pasji i zainteresowań.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Młodzieżowa Rada Gminy Suchy Las \n",
      "entering link http://chludowo.inton.info/\n",
      "W październiku 2016 r. Gmina Suchy Las złożyła wniosek o dofinansowanie projektu związanego z kompleksową termomodernizacją budynku Zespołu Szkół w Chludowie. Projekt przeszedł pozytywną ocenę strategiczną, formalną i merytoryczną i decyzją Zarządu Województwa Wielkopolskiego otrzymał dofinansowanie w wysokości 85% kosztów kwalifikowanych.\n",
      "W czerwcu 2017 r.  Wójt Gminy podpisał umowę o dofinansowanie.\n",
      "Realizacja inwestycji polegała na zmniejszeniu zapotrzebowania na energię dostarczaną na potrzeby ogrzewania, oświetlenia, ograniczenia strat energii i redukcję zanieczyszczeń powietrza poprzez obniżenie ilości substancji zanieczyszczających, wytwarzanych w procesie energetycznego spalania paliwa. Projekt obejmował rozwiązania zmierzające do poprawy stanu technicznego budynku, warunków cieplnych i zwiększenia energooszczędności oraz poprawy komfortu użytkowników.\n",
      "W zakres prac wchodziło m.in.: docieplenie obiektu, wymiana stolarki okiennej i drzwiowej, wymiana instalacji oświetlenia na energooszczędne, wymiana grzejników i montaż paneli fotowoltaicznych.  _ \n",
      "Całkowita wartość Projektu, to 3.178.275,01 zł., natomiast kwota dofinansowania z Unii Europejskiej, to  2.701.533,75 zł.\n",
      "Projekt współfinansowany przez Unię Europejską z Europejskiego Funduszu Rozwoju Regionalnego w ramach Wielkopolskiego Regionalnego Programu Operacyjnego na lata 2014-2020\n",
      "FUNDUSZE EUROPEJSKIE – LEPSZA JAKOŚĆ ŻYCIA W WIELKOPOLSCE\n",
      "Warsztaty filmowo-plastyczne, których efektem jest film animowany poruszający temat wpływu środowiska na proces uzależnienia jednostki, sposobów minimalizowania negatywnych czynników oraz roli szkoły i środowiska szkolno-wychowawczego w kształtowaniu pozytywnych zachowań.Uczestnicy wzięli udział w rozmowie na powyższy temat, a następnie stworzyli scenariusz z uwzględnieniem kreatywnego wykorzystania przestrzeni i znajdujących się w niej przedmiotów oraz film animowany w technikach animacji poklatkowej (piksilacja – animacja aktora, animacja przedmiotów znalezionych). Fabuła filmu zbudowana jest wokół metafory światła, jakopozytywnego wzorca postaw i zachowań, oraz cienia, jako szkodliwego otoczenia wpływającego na rozwój uzależnień. Szczególny nacisk położony został na kreowanie pozytywnego wizerunku szkoły jako przestrzeni sprzyjającej zdrowemu trybowi życia. Dokonane to zostało poprzez jej animizację, jako żywej, przychylnej i przyjaznej przestrzeni, w której przedmioty, takie jak piłka   sali gimnastycznej czy książki w bibliotece, same wychodzą naprzeciw uczniom, zachęcając ich i wspierając w rozwijaniu pasji i zainteresowań.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Młodzieżowa Rada Gminy Suchy Las \n",
      "entering link http://chludowo.pl/category/o-szkole/\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Młodzieżowa Rada Gminy Suchy Las \n",
      "entering link http://zsch.edupage.org\n",
      "Powered by aSc EduPage\n",
      "entering link http://chludowo.pl/wspolpraca-organizacji/\n",
      "W środowisku lokalnym odgrywa ważną rolę. Prowadzi ożywioną działalność wychowawczą wśród dzieci i młodzieży, należy do niej wielu uczniów naszej szkoły. Na swoich sztandarach ma wypisaną starą maksymę: „Bogu na chwałę, ludziom na pożytek”.\n",
      "W pracach OSP zaangażowany od kilkunastu lat jest nauczyciel naszej szkoły Marian Bajer.\n",
      "Straże pożarne, z którymi współpracujemy:\n",
      "Działa od 2003 roku. Było organizatorem i współorganizatorem znaczących imprez kulturalnych o zasięgu gminnym i ponadgminnym:\n",
      "ostatnio współpracujemy w ramach projektu „Edukacja dla Demokracji” realizowanego w naszej szkole.\n",
      "W pracach stowarzyszenia zaangażowani są nauczyciele z naszej szkoły: Maria Bajer jako prezes, Danuta Kamińska i Marian Bajer.\n",
      "Stowarzyszenie prowadzi działalność wydawniczą, publikując „Arkusz Golęczewski”. Dotychczas ukazały się cztery tomy, skoncentrowane na opisie przeszłości i teraźniejszości naszej gminy i okolicy (Krainy miedzy Wartą a Samicą). Pozycje te można wykorzystać w szkolnej pracy dydaktyczno-wychowawczej: nr 1 „Bibliografia o Krainie miedzy Wartą a Samicą”, oprac. Marian Bajer; nr 2 „Wojciech Bogusławski in memoriam” oprac.: Maria Bajer i Danuta Kamińska; nr 3 „Jubileusz 2005. Golęczewo”, oprac. Maria Bajer; nr 4 „Edukacja dla Demokracji. 20 lat demokracji w gminie Suchy Las”, oprac. Maria Bajer.\n",
      "Powstała w 1992 roku w Poznaniu z inicjatywy środowiska naukowego Akademii Medycznej im. Karola Marcinkowskiego w Poznaniu. Celem powołania Fundacji jest stworzenie profesjonalnego zaplecza dla polskich misjonarzy prowadzących działalność medyczną wśród chorych w najuboższych krajach świata. Fundacja zajmuje się także propagowaniem działalności humanitarnej i charytatywnej wśród młodzieży szkolnej i akademickiej\n",
      "Od 1997 roku zrzeszona jest w Medicus Mundi International – międzynarodowej sieci organizacji humanitarnych współpracujących ze sobą w dziedzinie opieki zdrowotnej na świecie, która z kolei jest w oficjalnych stosunkach ze Światową Organizacją Zdrowia.\n",
      "Swoją działalnością objęła przez lata istnienia kilkadziesiąt misyjnych szpitali, przychodni i punktów medycznych. Wolontariusze fundacji wspierali misyjną działalność o. Mariana Żelazka w Puri w Indiach Za pośrednictwem fundacji wspomagaliśmy naszego Patrona wysyłką lekarstw oraz środków opatrunkowych (szczególnie przygotowywanymi  przez naszych uczniów bandażami). W ostatnich latach uczestniczyliśmy w organizowanych przez fundację projektach ,,Puszka dla maluszka” i ,,Ołówek dla Afryki”. Gościliśmy w naszej szkole pracowników i wolontariuszy Fundacji oraz prezentowane były fotografie na okolicznościowych wystawach podczas szkolnych uroczystości.\n",
      "W latach 60-tych amerykański lekarz Hunter Cambell Adams zwany również Patchem Adamsem zainicjował oryginalną ideę terapii śmiechem. W przebraniu klauna rozweselał pacjentów w szpitalu. Głęboko wierzył, że zadaniem lekarza nie jest jedynie leczyć pacjenta ale również podnosić jakość jego życia. Głosił ideę, że lekarze powinni leczyć ludzi, nie choroby. Przekonywał lekarzy, że śmiech, radość są bardzo potrzebne chorym ludziom, a dzieciom w szczególności.\n",
      "Od tego czasu na całym świecie powstało wiele stowarzyszeń i fundacji wspierających i realizujących terapię śmiechem. Pracownicy i wolontariusze przebrani za Doktorów Clownów wzorem Patcha Adamsa propagują terapię śmiechem.\n",
      "Od 1999 roku terapia śmiechem jest realizowana również w Polsce , także przez oddział w Poznaniu. Zorganizowane zostały  zespoły terapeutów „doktorów clownów”, którzy w kolorowych strojach i makijażu, obowiązkowo z czerwonym noskiem niosą małym pacjentom wspaniałą zabawową terapię. Wolontariusze zabawiając uczestników imprez, festynów i uroczystości kwestują także na rzecz podopiecznych . Podczas festynu rodzinnego  w 2010 r. organizowanego przez naszą Radę Rodziców zbierali pieniądze na leczenie chorego Mikołaja z Zielątkowa.\n",
      "Animatorzy  poznańskiego oddziału dr Clown pojawili się też na półkoloniach  zorganizowanych dla naszych uczniów  w lipcu 2010 r. Uczestniczyli w projekcie Wolontariat dla każdego. Relację z tego spotkania zrejestrowała telewizja internetowa . Można ją zobaczyć na www.tvsl.pl\n",
      "Celem Stowarzyszenia jest świadczenie pomocy zamieszkałym na terenie gminy Suchy Las dzieciom: pochodzącym z rodzin najuboższych; z rodzin wielodzietnych; niepełnosprawnym; poszkodowanym w wyniku wypadków losowych oraz dzieciom szczególnie utalentowanym.\n",
      "SSPD realizuje swoje cele przez świadczenie na rzecz dzieci  pomocy finansowej i rzeczowej, finansowanie różnych form kształcenia; organizowanie i finansowanie różnych form wypoczynku i rekreacji, finansowanie leczenia, rehabilitacji; podejmowanie działań umożliwiających dzieciom  integrowanie się z rówieśnikami.\n",
      "Z nasza szkołą Sucholeskie Stowarzyszenie Pomocy Dzieciom związane jest od momentu powstania, od 1997r. Młodzi mieszkańcy Chludowa wielokrotnie mieli okazje dzięki wsparciu otrzymanemu od Stowarzyszenia zwiedzać Hiszpanię, Słowację, Czechy czy Niemcy. Wielokrotnie uczestniczyli w koloniach letnich w Polsce: w Pobierowie, Pogorzelicy, Rewalu, w Chalinie i na Kaszubach. Dofinansowywane było również szkolenie tancerzy zespołu ludowego ,,Chludowianie”.\n",
      "Przedszkolaki z Chludowa i Golęczewa wyposażone zostały w odblaskowe kamizelki, wręczane w ramach projektu ,, Dbamy o bezpieczeństwo dzieci” 1 czerwca 2009. SSPD dofinansowywało rehabilitację chorej uczennicy szkoły podstawowej w Chludowie  oraz  wsparło finansowo leczenie przewlekle chorych uczniów. Od kilku lat organizowane są Turnieje Piłki Halowej – Gwiazdkowy i Wielkanocny – promujący zdrowy styl życia  Najważniejszym projektem Stowarzyszenia jest jednak ,, Niespodzianka od św. Mikołaja” , kiedy to dla wszystkich dzieci z gminy przygotowane są słodycze, a wręczane one są przez Św. Mikołaja.\n",
      "Pieniądze na działalność Stowarzyszenie pozyskuje dzięki wsparciu sponsorów i z darowizn – w tym  z 1% podatku naszych darczyńców, a także dzięki organizacji Bali Charytatywnych i Rodzinnych Rajdów Samochodowych.  Imprezom tym honorowo patronuje zawsze Wójt gminy Suchy Las Grzegorz Wojtera\n",
      "Instytucja ta realizuje swoje zadania statutowe, niosąc pomoc dzieciom i młodzieży z naszej szkoły poprzez wsparcie finansowe potrzebujących rodzin oraz finansowanie szkolnych obiadów i posiłków regeneracyjnych. Od 2000 roku, organizowane SA dla naszych uczniów półkolonie.  Odpłatność za uczestnictwo dziecka, które jest mieszkańcem gminy wynosi 20 złotych, . Kierownicy odpowiedzialni za układanie programów oraz ich realizacje dbają o to, aby dzieci nie nudziły się w czasie zajęć. Dlatego też wyjeżdżają one na basen, kręgle, do kina. Zwiedzają parki dinozaurów, palmiarnie, zoo. Z roku na rok programy urozmaicane są coraz to nowymi atrakcjami.\n",
      "Uczniowie naszej szkoły od 2009 r. uczestniczą w zajęciach ,,Uniwersytetu dla dzieci”\n",
      "Jest to projekt przygotowany przez WSNHiD w Poznaniu i adresowany do dzieci w przedziale od 6 – 12 lat. Oferta ciekawych wykładów ma na celu przybliżenie dzieciom różnych dziedzin nauki, obudzenie w nich zainteresowań oraz pasji zdobywania wiedzy. Przy okazji dzieci zapoznają się z przestrzenia akademicką , otrzymują indeks i dyplomy.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Młodzieżowa Rada Gminy Suchy Las \n",
      "entering link https://synergia.librus.pl/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rekrutacja do szkół ponadpodstawowych\n",
      "#szkoła\n",
      "\n",
      "\n",
      "Jak motywować i oceniać – pigułka wiedzy na najbliższy okres\n",
      "#rozwój\n",
      "\n",
      "\n",
      "Matura z języka angielskiego - arkusz egzaminacyjny\n",
      "#szkoła\n",
      "\n",
      "\n",
      "\n",
      "                                Matura z matematyki – arkusz egzaminacyjny\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "                                Matura z języka polskiego – arkusz egzaminacyjny\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "                                Powrót do edukacji stacjonarnej w maju - szczegóły rozporządzenia\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "                                Jak tegoroczni absolwenci odbiorą świadectwa\n",
      "                            \n",
      "\n",
      "\n",
      "W maju uczniowie wracają do szkół\n",
      "#szkoła\n",
      "\n",
      "\n",
      "Rodzinna majówka w domowym zaciszu? Postaw na planszówki!\n",
      "#czas wolny\n",
      "\n",
      "\n",
      "Nauka od 26 kwietnia br. – szczegóły rozporządzenia\n",
      "#szkoła\n",
      "\n",
      "\n",
      "Częściowy powrót do szkół w niektórych województwach\n",
      "#szkoła\n",
      "\n",
      "\n",
      "Nadużywanie Internetu – jak pomóc dziecku być off-line! 2/2\n",
      "#kampania społeczna\n",
      "\n",
      "\n",
      "Problem nadużywania urządzeń ekranowych w czasie pandemii 1/2\n",
      "#kampania społeczna\n",
      "\n",
      "\n",
      "\n",
      "                                Kontynuacja nauki zdalnej – znamy szczegóły\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "                                Nauczanie zdalne zostaje przedłużone\n",
      "                            \n",
      "\n",
      "\n",
      "Zmień swoje nawyki na dobre – dowiedz się, jak wpoić je dziecku\n",
      "#wychowanie\n",
      "\n",
      "\n",
      "Miłość od pierwszego ugryzienia\n",
      "#ciekawostki\n",
      "\n",
      "\n",
      "Dzieci potrzebują się smucić\n",
      "#psychologia\n",
      "\n",
      "\n",
      "NOWOŚĆ – wyszukuj treści, załączaj zadania domowe\n",
      "#aplikacja Librus\n",
      "\n",
      "\n",
      "Przedłużono nauczanie zdalne dla wszystkich uczniów\n",
      "#szkoła\n",
      "\n",
      "\n",
      "Autyści są wśród nas – co na ich temat wiemy\n",
      "#społeczeństwo\n",
      "\n",
      "\n",
      "\n",
      "                                Senne poranki? Zobacz, jak budzić dziecko\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "                                Przedszkola i żłobki zostają zamknięte\n",
      "                            \n",
      "\n",
      "\n",
      "Wielkanocne ozdoby – zrób je samodzielnie!\n",
      "#wielkanoc\n",
      "\n",
      "\n",
      "Praca zawodowa a opieka nad dzieckiem [ANKIETA UJ]\n",
      "#ankieta\n",
      "\n",
      "\n",
      "Próbny egzamin ósmoklasisty – arkusze i odp. z j. angielskiego\n",
      "#egzmainy\n",
      "\n",
      "\n",
      "Powrót nauki zdalnej – znamy szczegóły\n",
      "#szkoła\n",
      "\n",
      "\n",
      "Próbny egzamin ósmoklasisty – arkusze i odpowiedzi z matematyki\n",
      "#egzaminy\n",
      "\n",
      "\n",
      "Wraca edukacja zdalna dla wszystkich uczniów\n",
      "#szkoła\n",
      "\n",
      "\n",
      "\n",
      "                                Próbny egzamin ósmoklasisty – arkusze i odpowiedzi z j. polskiego\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "                                Szczegóły dot. nauczania hybrydowego – rozp. podpisane\n",
      "                            \n",
      "\n",
      "\n",
      "Liczba Pi niejedno ma oblicze!\n",
      "#święto\n",
      "\n",
      "\n",
      "Zmiany w edukacji w kolejnych województwach\n",
      "#szkoła\n",
      "\n",
      "\n",
      "Zrób coś dla siebie – rozwój jest ważny nie tylko u dziecka\n",
      "#rozwój\n",
      "\n",
      "\n",
      "Kobiety, które osiągnęły to, co pierwotnie niemożliwe\n",
      "#święto\n",
      "\n",
      "\n",
      "Matura próbna – arkusze i odpowiedzi z języka angielskiego\n",
      "#matura\n",
      "\n",
      "\n",
      "Matura próbna – arkusze i odpowiedzi z matematyki\n",
      "#matura\n",
      "\n",
      "\n",
      "        Ta strona wykorzystuje pliki cookies. Za ich pomocą zbierane są informacje, które mogą stanowić dane osobowe. Wykorzystujemy je m.in. w celach statystycznych i funkcjonalnych. Korzystając z serwisu bez zmiany konfiguracji przeglądarki, wyrażasz zgodę na zapisanie plików cookies w pamięci Twojego urządzenia. Możesz samodzielnie zarządzać cookies zmieniając odpowiednio ustawienia w Twojej przeglądarce. Więcej informacji o zasadach przetwarzania Twoich danych osobowych oraz przysługujących Ci prawach znajdziesz w Polityce prywatności.\n",
      "    \n",
      "\n",
      "                            Mobilne dodatki do aplikacji Librus to funkcje zwiększające komfort korzystania z niej.\n",
      "                            Wiadomości, Zadania domowe, widok Biurko to tylko część z udogodnień, z których możesz\n",
      "                            korzystać. Ciesz się wygodą na smartfonie, tablecie i komputerze.\n",
      "\n",
      "                            Mobilne dodatki do aplikacji Librus to funkcje zwiększające komfort korzystania z niej.\n",
      "                            Wiadomości, Zadania domowe, widok Biurko to tylko część z udogodnień, z których możesz\n",
      "                            korzystać. Ciesz się wygodą na smartfonie, tablecie i komputerze.\n",
      "                        \n",
      "\n",
      "                            Mobilne dodatki do aplikacji Librus to funkcje zwiększające komfort korzystania z niej.\n",
      "                            Wiadomości, Zadania domowe, widok Biurko to tylko część z udogodnień, z których możesz\n",
      "                            korzystać. Ciesz się wygodą na smartfonie, tablecie i komputerze.\n",
      "                        \n",
      "\n",
      "                            Pakiet testów „Mocne strony i predyspozycje” oraz 3-letni dostęp do portalu z bazą metod,\n",
      "                            technik i kart pracy podnoszących efektywność nauki.\n",
      "                        \n",
      "Cena regularna\n",
      " Konto LIBRUS\n",
      "Zapisanie wybranej przez Ciebie roli (rodzic, uczeń/ pracownik szkoły) w pliku cookies umożliwi jej zapamiętanie. W przyszłości dla Twojej wygody i oszczędności czasu przejdziesz bezpośrednio do wybranego portalu z pominięciem tego kroku.\n",
      "\n",
      "                    Pamiętaj! – Portale Librus nie są częścią szkolnego systemu Synergia.\n",
      "                \n",
      "Zostałeś wylogowany(-a) z Konta LIBRUS na wszystkich Twoich urządzeniach.\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "website='http://chludowo.pl'\n",
    "html=urlopen(website)\n",
    "soup=BeautifulSoup(html.read())\n",
    "\n",
    "for paragraph in soup.find_all('p'):\n",
    "    print(paragraph.get_text())\n",
    "\n",
    "\n",
    "num=0\n",
    "for link in soup.find_all('a'):\n",
    "    \n",
    "    if(link.get('href')==None):break\n",
    "    \n",
    "    print(\"entering link {}\".format(link.get('href')))\n",
    "    \n",
    "    inner_html = urlopen(urljoin(website,link.get('href')))\n",
    "    inner_soup=BeautifulSoup(inner_html.read())\n",
    "    \n",
    "    for paragraph in inner_soup.find_all('p'):\n",
    "        print(paragraph.get_text())\n",
    "        \n",
    "    if(num==5):break\n",
    "    else: num+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 - CONLL\n",
    "Dane ustrukturyzowane w formacie CONLL.\n",
    "\n",
    "Niektóre algorytmy korzystają z dodatkowych metadanych opisujących poszczególne tokeny (słowa). Bardzo popularnym formatem zapisu takich danych jest format CONLL. \n",
    "\n",
    "Reprezentacja CONLL polega na tym, że dany tekst dzielony jest na zdania, a następnie każde zdanie dzielone jest na tokeny (tokenizowane). Następnie dla każdego tokenu tworzymy listę opisującą cechy tego tokenu (słowa).\n",
    "Poniżej przykład wektora opisującego każdy token zadanego tekstu:\n",
    "<ol>\n",
    "    <li>ID - numer porządkowy tokenu w zdaniu</li>\n",
    "    <li>text - tekst tokenu w formie nieprzetworzonej</li>\n",
    "    <li>Part of Speech tag (POS tag) - informacja o części mowy, która powiązana jest z tym słowem </li>\n",
    "    <li>is digit - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest liczbą</li>\n",
    "    <li>is punct - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest znakiem interpunkcyjnym</li>\n",
    "</ol>\n",
    "\n",
    "Wektory cech dla kolejnych słów zapisywane są pod sobą. **Separatorem cech w wektorze jest pojedyncza spacja.**\n",
    "\n",
    "**Zdania zwyczajowo oddzielamy od siebie podwójnym znakiem nowej linii.**\n",
    "\n",
    "Historycznie CONLL był bardzo konkretnym formatem danych w którym mieliśmy z góry narzucone cechy (np. format CONLL-U https://universaldependencies.org/docs/format.html). Liczba cech ewoluowała jednak w czasie i w wielu miejscach CONLL stał się synonimem ogólnego formatu, w którym dobór cech zależy tylko od nas, jednak stałym jest zapis sekwencji tokenów jako sekwencji wierszy w tekście, gdzie każdy wiersz jest listą oddzielonych spacją wartości (cech), a zdania oddzielone są od siebie podwójnym znakiem nowej linii.\n",
    "\n",
    "\n",
    "### Przykład:\n",
    "\n",
    "Tekst: Kasia kupiła 2 lizaki: truskawkowy i zielony. Kasia używa Apple IPhone 5 i IPad.\n",
    "\n",
    "Reprezentacja CONLL **(spacje separujące kolumny zostały zwielokrotnione na potrzeby zwiększenia czytelności)**\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 kupiła CZASOWNIK  0 0\n",
    "3 2      LICZEBNIK  1 0\n",
    "4 lizaki RZECZOWNIK 0 0\n",
    "5 .      _          0 1\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 używa  CZASOWNIK  0 0\n",
    "3 Apple  RZECZOWNIK 0 0\n",
    "4 IPhone RZECZOWNIK 0 0\n",
    "5 5      LICZEBNIK  1 0\n",
    "6 i      SPÓJNIK    0 0\n",
    "7 iPad   RZECZOWNIK 0 0\n",
    "8 .      _          0 1\n",
    "</pre>\n",
    "\n",
    "**Zadanie 2a (0.5 pkt)**: Napisz funkcję, która z zadanego tekstu w formie surowego tekstu stworzy reprezentację CONLL opisaną wcześniej wymienionymi atrybutami (ID, text, POS-tag, is_digit, is_punct).\n",
    "\n",
    "Wykorzystaj sentence splitter i tokenizator z NLTK. Do uzyskania informacji o POS-tagach każdego tokenu wykorzystaj funkcję nltk.pos_tag(). W kolumnie związanej z POS-tagiem zapisz pos tag w takiej formie, w jakiej uzyskamy go z funkcji pos_tag (pos_tag() zwraca formy skrótowe, np. 'NN' dla rzeczowników), nie trzeba więc zamieniać napisu \"NN\" na \"RZECZOWNIK\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Kate NNP 0 0\n",
      "2 uses VBZ 0 0\n",
      "3 IPhone NNP 0 0\n",
      "4 5 CD 1 0\n",
      "5 and CC 0 0\n",
      "6 IPad NNP 0 0\n",
      "7 . . 0 1\n",
      "\n",
      "1 Kate NNP 0 0\n",
      "2 bought VBD 0 0\n",
      "3 2 CD 1 0\n",
      "4 lolipops NNS 0 0\n",
      "5 . . 0 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def generate_conll(text):\n",
    "\n",
    "    for s in sent_tokenize(text):\n",
    "        i=1\n",
    "        for word,tag in pos_tag(word_tokenize(s)):\n",
    "            print(i,word,tag,int(tag=='CD'),int(tag in '.,?!'))\n",
    "            i+=1\n",
    "        print()\n",
    "    \n",
    "\n",
    "generate_conll(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Wyobraźmy sobie teraz, że chcielibyśmy wykrywać wzmianki o urządzeniach elektronicznych w tekście. W jaki sposób zakodować informację o (potencjalnie wielotokenowych) nazwach produktów w CONLL, tak, aby później móc wykonać proces uczenia?\n",
    "\n",
    "Dodajmy w naszym CONLLu dodatkową kolumnę reprezentującą informację o urządzeniach elektronicznych.\n",
    "Nazwy urządzeń mogą składać się potencjalnie z wielu słów.\n",
    "Do zakodowania wielotokenowych tekstów używa się najczęściej notacji IOB, gdzie każda literka skrótu oznacza interpretację aktualnego słowa:\n",
    "<ul>\n",
    "    <li> B = begin, marker, który mówi, że aktualne słowo to początek nazwy </li>\n",
    "    <li> I = inside, marker, który mówi, że aktualne słowo to kontynacja nazwy, która rozpoczyna się wystąpieniem wcześniejszego B</li>\n",
    "    <li> O = outside, marker, który mówi, że aktualne słowo nie jest interesującą nas nazwą (jest poza nią) </li>\n",
    "</ul>\n",
    "\n",
    "Po dodaniu nowej kolumny (na końcu) nasz CONLL przybiera postać:\n",
    "\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 kupiła CZASOWNIK  0 0 O\n",
    "3 2                 1 0 O\n",
    "4 lizaki RZECZOWNIK 0 0 O\n",
    "5 .      _          0 1 O\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 używa             0 0 O\n",
    "3 Apple  RZECZOWNIK 0 0 B\n",
    "4 IPhone RZECZOWNIK 0 0 I\n",
    "5 5                 1 0 I\n",
    "6 i      SPÓJNIK    0 0 O\n",
    "7 iPad   RZECZOWNIK 0 0 B\n",
    "8 .      _          0 1 0\n",
    "</pre>\n",
    "\n",
    "Zwróćcie Państwo uwagę na ostatnią kolumnę, czytając tekst od góry w dół, wystąpienie literki \"B\" oznacza początek interesującej frazy (Apple), jeśli zaraz za \"B\" pojawia się sekwencja oznaczona jako \"I\" - kolejne tokeny stanowią kontynuację interesującej nas frazy, w tym przypadku 3 tokeny \"Apple IPhone 5\" tworzą jeden byt. Poza tym widzimy, że \"iPad\" stanowi osobny, jednotokenowy byt.\n",
    "\n",
    "Po co rozróżniać pomiędzy \"B\", \"I\" i \"O\", czy nie można uwzględnić tylko dwóch tagów \"wewnątrz frazy\", \"poza frazą\"? Teoretycznie można, ale wprowadzimy w ten sposób sytuacje niejednoznaczne. \n",
    "\n",
    "Sprawdźmy to na przykładzie sekwencji \"XBox Playstation\" reprezentującej 2 osobne byty. Używając tagowania IOB nasza sekwencja wyglądałaby tak:\n",
    "\n",
    "XBox B\n",
    "PlayStation B\n",
    "\n",
    "Widzimy więc, że dwa tagi \"B\" oznaczają dwa początki osobnych fraz. Co jednak gdybyśmy używali tagów \"wewnątrz (interesującej nas) frazy\", \"poza (interesującą nas) frazą\"?\n",
    "\n",
    "XBox \"wewnątrz (interesującej nas) frazy\"\n",
    "Playstation \"wewnątrz (interesującej nas) frazy\"\n",
    "\n",
    "W tej sytuacji oznaczyliśmy poprawnie oba tokeny jako części interesujących nas fraz. Jednak nie wiemy, czy XBox Playstation to jedna, czy dwie osobne frazy (byty) -- stąd format IOB jest zdecydowanie bezpieczniejszym wyborem.\n",
    "\n",
    "**Zadanie 2b (0.5 pkt)**: Napisz funkcję, która wygeneruje CONLL z uwzględnieniem tagów IOB dotyczących urządzeń.\n",
    "Nasza funkcja posiada teraz dodatkowy argument devices, który zawiera listę obiektów, które opisują gdzie (przesunięcie znakowe) znajduje się początek i koniec wzmianek.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Kate NNP 0 0 O\n",
      "2 uses VBZ 0 0 O\n",
      "3 IPhone NNP 0 0 B\n",
      "4 5 CD 1 0 I\n",
      "5 and CC 0 0 O\n",
      "6 IPad NNP 0 0 B\n",
      "7 . . 0 1 O\n",
      "1 Kate NNP 0 0 O\n",
      "2 bought VBD 0 0 O\n",
      "3 2 CD 1 0 O\n",
      "4 lolipops NNS 0 0 O\n",
      "5 . . 0 1 O\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def generate_CONLL(text, devices=[]):\n",
    "\n",
    "    for dev in devices:\n",
    "        dev['range']=range(dev['begin'],dev['end'])\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for s in doc.sents:\n",
    "        i=1\n",
    "        for w in s:\n",
    "            \n",
    "            iob='O'\n",
    "            for dev in devices:\n",
    "                if(w.idx==dev['begin']):\n",
    "                    iob='B'\n",
    "                    break\n",
    "                elif (w.idx in dev['range']):\n",
    "                    iob='I'\n",
    "                    break\n",
    "                    \n",
    "            print(i,w.text,w.tag_,int(w.like_num),int(w.is_punct),iob)\n",
    "            i+=1\n",
    "\n",
    "# parametr devices to lista słowników w którym mamy informację o numerze znaku na którym fraza się zaczyna i kończy (zobacz: próba wywołania w ostatniej linijce) (litera I z Iphone występuje na 10 znaku)\n",
    "# Zapoznaj się z dokumentacją SpaCy (obiekt Token), aby zobaczyć jak wydobyć informację o pozycji danego słowa w zdaniu/dokumencie.\n",
    "    \n",
    "generate_CONLL(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\", devices=[{\"begin\": 10, \"end\":18}, {\"begin\": 23, \"end\": 27}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Często chcemy w tekście naraz oznaczać byty, które należą do różnych kategorii, np. lokacje, numery telefonów, daty, wzmianki o osobach. W takich sytuacjach używa się również kodowania IOB jednak wzbogaca się etykiety o odpowiednie informacje używając formatu:\n",
    "\n",
    "{tag IOB}-{etykieta kategorii}\n",
    "\n",
    "Stąd daty przyjmują oznaczenia: B-DATE / I-DATE, osoby B-PERSON / I-PERSON, numery telefonów B-PHONENUMBER / I-PHONENUMBER, lokacje: B-LOCATION / I-LOCATION itp. Wiemy zatem czy dany token należy do interesującej nas frazy i do jakiej kategorii przypisana jest ta fraza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykrywanie encji nazwanych (Named Entity Recognition - NER)\n",
    "\n",
    "Dotychczas na większości zajęć rozważaliśmy problem klasyfikacji, w którym całym dokumentom przypisywalśmy pojedynczą etykietę (sentyment związany z dokumentem, informacja o tym, czy tekst jest spamowy, etykieta mówiąca o tym w jakim języku napisany jest dokument). Warto jednak również wspomnieć o tzw. tagowaniu sekwencji, które dla każdego elementu sekwencji (słowa) nadaje odpowiednią etykietę.\n",
    "\n",
    "Gdzie taka procedura ma zastosowanie? Wymieńmy kilka przykładów \n",
    "<ol>\n",
    "    <li>Wykrywanie wyrażeń dotyczących miejsc, ludzi, czasu, lokalizacji itp. - każde kolejne słowo tagowane jest informacją mówiącą o tym, czy dane słowo jest częścią pożądanego przez nas typu (np. częścią lokalizacji), czy nie (np. z użyciem kodowania IOB, o którym mówiliśmy przy okazji CONLL)</li>\n",
    "    <li>Tagowanie częściami mowy - każde słowo otrzymuje etykietę mówiącą o tym jaka część mowy reprezentowana jest przez aktualny token.</li>\n",
    "    <li>Wykrywanie ważnych z naszego punktu widzenia fraz (nazwy produktów, technologii itp.)</li>\n",
    "    <li>...</li>\n",
    "</ol>\n",
    "\n",
    "Mówiąc o encjach nazwanych (Named Entities) - mówimy o frazach, którym nadaliśmy określony typ, np: \"01.06.2018\" - typ \"Data\", \"Poznań, Polska\" - typ \"Lokalizacja\", \"GeForce 1080 GTX Ultra\" - typ \"Sprzęt Komputerowy\".\n",
    "\n",
    "\n",
    "\n",
    "## Własny NER - trening z użyciem algorytmu CRF (Conditional Random Fields)\n",
    "\n",
    "Wykrywacze encji wytrenowane są do odnajdywania popularnych typów fraz (Daty, Lokalizacje, Osoby, ...). Co jednak, kiedy chcielibyśmy wykrywać zdefiniowane przez nas typy danych (np. sprzęt komputerowy), które nie są domyśnie wspierane przez istniejące modele? Musielibyśmy wytrenować własnego NERa. Użyjmy paczki 'pycrfsuite' do tego celu.\n",
    "\n",
    "PyCRFSuite implementuje algorytm CRF - bardzo wydajny algorytm, który potrafi uczyć się tagowania poszczególnych słów z użyciem np. kodowania IOB. Aby rozróżnić różne rodzaje encji, często tagi \"I\" i \"B\" kodowania IOB opatruje się dodatkowym sufiksem. Np. B-Date - oznacza początek daty, a I-Location - kontynuację frazy zawierającej lokację.\n",
    "\n",
    "Ponieważ to czy dane słowo jest encją nazwaną zależy zarówno od tego jak dane słowo wygląda, jak i od słów poprzedzających i następujących po aktualnym - w opisie cech CRFów również uwzględnia się informacje o okalających słowach.\n",
    "\n",
    "**Zadanie 3 (2 punkty)** Wytrenuj model, który będzie tagował poszczególne słowa w tekście z użyciem pycfrsuite. Aby to zrobić, wykonaj podzadania w krokach poniżej.\n",
    "\n",
    "Nasz NER będzie się uczyć etykiet na zbiorze tekstów hiszpańskich, które poddane są podziałowi na zdania, tokenizacji, tagowaniem częściami mowy i etykietami encji do wykrycia w formacie IOB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-crfsuite in c:\\users\\user\\anaconda3\\lib\\site-packages (0.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2002.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('El', 'DA', 'O'),\n",
       " ('Abogado', 'NC', 'B-PER'),\n",
       " ('General', 'AQ', 'I-PER'),\n",
       " ('del', 'SP', 'I-PER'),\n",
       " ('Estado', 'NC', 'I-PER'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('Daryl', 'VMI', 'B-PER'),\n",
       " ('Williams', 'NC', 'I-PER'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('subrayó', 'VMI', 'O'),\n",
       " ('hoy', 'RG', 'O'),\n",
       " ('la', 'DA', 'O'),\n",
       " ('necesidad', 'NC', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('tomar', 'VMN', 'O'),\n",
       " ('medidas', 'NC', 'O'),\n",
       " ('para', 'SP', 'O'),\n",
       " ('proteger', 'VMN', 'O'),\n",
       " ('al', 'SP', 'O'),\n",
       " ('sistema', 'NC', 'O'),\n",
       " ('judicial', 'AQ', 'O'),\n",
       " ('australiano', 'AQ', 'O'),\n",
       " ('frente', 'RG', 'O'),\n",
       " ('a', 'SP', 'O'),\n",
       " ('una', 'DI', 'O'),\n",
       " ('página', 'NC', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('internet', 'NC', 'O'),\n",
       " ('que', 'PR', 'O'),\n",
       " ('imposibilita', 'VMI', 'O'),\n",
       " ('el', 'DA', 'O'),\n",
       " ('cumplimiento', 'NC', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('los', 'DA', 'O'),\n",
       " ('principios', 'NC', 'O'),\n",
       " ('básicos', 'AQ', 'O'),\n",
       " ('de', 'SP', 'O'),\n",
       " ('la', 'DA', 'O'),\n",
       " ('Ley', 'NC', 'B-MISC'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install python-crfsuite\n",
    "nltk.download('conll2002')\n",
    "import nltk\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train')) # załaduj korpus treningowy dla języka hiszpańskiego\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))  # załaduj korpus testowy dla języka hiszpańskiego\n",
    "train_sents[2] # wyświetla przykładowe zdanie, aby zobaczyć jak reprezentowane są dane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 3a (1 punkt)** Tworzenie cech. PyCRFSuite oczekuje, że każde słowo opisane będzie zestawem odpowiednich cech w formie pythonowego słownika. Uzupełnij kod funkcji word2features (sekcje TODO) tak, aby stworzyć odpowiednie cechy zgodnie z nazwami i komentarzami do poszczególnych pól."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]  # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'Ley'\n",
    "    postag = sent[i][1] # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'NC'\n",
    "    \n",
    "    features = {      # cechy aktualnego słowo\n",
    "        'bias': 1.0,\n",
    "        'lowercase_word': word.lower(), # TODO, tutaj słowo małymi literami\n",
    "        'word_last_3_chars': word[-3:], # TODO, tutaj ostatnie 3 znaki słowa\n",
    "        'word_last_2_chars': word[-2:], # TODO, tutaj ostatnie 2 znaki słowa\n",
    "        'word_is_uppercase': word.isupper(), # TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
    "        'word_is_digit': word.isdigit(), # TODO, tutaj flaga (True/False), czy słowo jest liczbą\n",
    "        'postag': postag, # TODO, tutaj pos-tag (patrz początek definicji funkcji)\n",
    "        'postag_first_two_chars': postag[:2] # TODO, tutaj pierwsze 2 znaki pos-tagu  \n",
    "    }\n",
    "    if i > 0:         # jeśli nasze słowo nie jest pierwszym w zdaniu - dodajmy do zbioru naszych cech cechy poprzedniego tokenu\n",
    "        word1 = sent[i-1][0]    # poprzednie słowo\n",
    "        postag1 = sent[i-1][1]  # poprzedni pos-tag\n",
    "        \n",
    "        features.update({       # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
    "            'previous_word_lower':word1.lower(), # TODO, tutaj poprzednie słowo małymi literami\n",
    "            'previous_word_is_upppercase': word1.isupper(),# TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
    "            'previous_word_postag':postag1, # TODO, tutaj pos-tag poprzedniego słowa \n",
    "            'previous_word_postag_first_two_chars':postag1[:2] # TODO, tutaj pierwsze 2 znaki pos-tagu  poprzedniego słowa\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True   # jeśli to pierwszy token - ustawmy cechę BOS (Begin of Sentence) na True\n",
    "        \n",
    "    if i < len(sent)-1:          # Jeśli nasze słowo nie jest ostatnim - dodajmy do zbioru cech cechy następnego słowa \n",
    "        word1 = sent[i+1][0]     # następne słowo\n",
    "        postag1 = sent[i+1][1]   # następny postag\n",
    "        \n",
    "        features.update({        # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
    "            'next_word_is_lower': word1.islower(),# TODO, tutaj flaga - czy następne słowo małymi literami\n",
    "            'next_word_is_upppercase': word1.isupper(), # TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
    "            'next_word_postag': postag1,# TODO, tutaj pos-tag następnego słowa \n",
    "            'next_word_postag_first_two_chars':postag1[:2] # TODO, tutaj pierwsze 2 znaki pos-tagu  następnego słowa\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True   # jeśli to ostatni token - ustawmy cechę EOS (End of Sentence) na True\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))] # zamień każde słowo ze zdania na słownik cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'lowercase_word': 'melbourne',\n",
       " 'word_last_3_chars': 'rne',\n",
       " 'word_last_2_chars': 'ne',\n",
       " 'word_is_uppercase': False,\n",
       " 'word_is_digit': False,\n",
       " 'postag': 'NP',\n",
       " 'postag_first_two_chars': 'NP',\n",
       " 'BOS': True,\n",
       " 'next_word_is_lower': False,\n",
       " 'next_word_is_upppercase': False,\n",
       " 'next_word_postag': 'Fpa',\n",
       " 'next_word_postag_first_two_chars': 'Fp'}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczekiwany rezultat: \n",
    "<pre>\n",
    "{'BOS': True,\n",
    " 'bias': 1.0,\n",
    " 'lowercase_word': 'melbourne',\n",
    " 'next_word_lower': 'False',\n",
    " 'next_word_is_upppercase': False,\n",
    " 'next_word_postag': 'Fpa',\n",
    " 'next_word_postag_first_two_chars': 'Fp',\n",
    " 'postag': 'NP',\n",
    " 'postag_first_two_chars': 'NP',\n",
    " 'word_is_digit': False,\n",
    " 'word_is_uppercase': False,\n",
    " 'word_last_2_chars': 'ne',\n",
    " 'word_last_3_chars': 'rne'}\n",
    "</pre>\n",
    " \n",
    " **Zadanie 3b (1 punkt) - napisz ciała funkcji pomocniczych, które dla aktualnego zdania z train_sents i test_sents zwrócą:**\n",
    " <ul>\n",
    "     <li>sent2labels - zwróci ciąg oczkiwanych etykiet dla każdego wyrazu. parametr sent jest listą słów, z których każde słowo opisane jest trójką: tekst słowa, pos-tag słowa, etykieta słowa; np. ('Abogado', 'NC', 'B-PER') </li>\n",
    "     <li>sent2tokens - analogicznie do powyższego, jednak zamiast etykiet zwróci ciąg słów bez pos-tagów i etykiet.</li>\n",
    "     <li>get_all_labels - funkcja, która ze zbioru wszystkich zdań treningowych wyświetli zbiór etykiet (zbiór, czyli bez powtórzeń). Funkcja pokaże nam ilu etykiet chcemy się nauczyć, aby móc ocenić trudność naszego problemu.</li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
      "{'I-LOC', 'B-PER', 'O', 'I-ORG', 'I-PER', 'B-ORG', 'I-MISC', 'B-LOC', 'B-MISC'}\n"
     ]
    }
   ],
   "source": [
    "def sent2labels(sent):\n",
    "    return [elem[2] for elem in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [elem[0] for elem in sent]\n",
    "\n",
    "def get_all_labels(train_sents):\n",
    "    return {elem[2] for sentence in train_sents for elem in sentence }\n",
    "print(sent2labels(train_sents[0]))\n",
    "print(sent2tokens(train_sents[0]))\n",
    "print(get_all_labels(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczekiwany rezultat:\n",
    "<pre>\n",
    "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
    "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
    "{'I-PER', 'I-MISC', 'B-LOC', 'I-LOC', 'B-PER', 'B-MISC', 'I-ORG', 'B-ORG', 'O'}\n",
    "</pre>\n",
    "\n",
    "Uruchom poniższy kod i sprawdź czego nauczył się nasz NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Coruña , 23 may ( EFECOM ) .\n",
      "\n",
      "Predicted: B-LOC I-LOC O O O O B-ORG O O\n",
      "Correct:   B-LOC I-LOC O O O O B-ORG O O\n"
     ]
    }
   ],
   "source": [
    "X_train = [sent2features(s) for s in train_sents] # Stwórz cechy zbioru treningowego\n",
    "y_train = [sent2labels(s) for s in train_sents]   # Pobierz etykiety zbioru treningowego\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]   # Stwórz cechy zbioru testowego\n",
    "y_test = [sent2labels(s) for s in test_sents]     # Pobierz etykiety zbioru testowego\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)    # stwórz obiekt trenujący\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):       # iteruj po zdaniach i etykietach\n",
    "    trainer.append(xseq, yseq)                 # dopisuj do obiektu trenującego nasze dane\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # parametr regularyzacyjny L1\n",
    "    'c2': 1e-3,  # parametr regularyzacyjny L2\n",
    "    'max_iterations': 50,  # maksymalna liczba iteracji\n",
    "    # dodaj tranzycje, które nie są obserwowane ale są możliwe\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train('conll2002-esp.crfsuite')       # wytrenuj model i zapisz do pliku!\n",
    "\n",
    "tagger = pycrfsuite.Tagger()                  # stwórz tagger, który będzie nadawał etykiety naszej sekwencji\n",
    "tagger.open('conll2002-esp.crfsuite')         # załaduj do niego wytrenowany model\n",
    "example_sent = test_sents[0]                  # weź pierwsze z brzegu zdanie, które nie brało udziału w treningu\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')   # wyświetl je...\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))  # zobacz, co generuje nasz model\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))                # i to, czego oczekiwano!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition za pomocą sieci neuronowych:\n",
    "\n",
    "Jeśli zastanawiacie się jak zrobić NERa za pomocą sieci neuronowych (Keras), to na Kaggle jest świetny fragment kodu: https://www.kaggle.com/ananysharma/ner-with-bi-lstm\n",
    "\n",
    "Po ostatnich zajęciach ten kod powinien być prosty do zrozumienia :)\n",
    "\n",
    "Bi-LSTM to dwukierunkowe LSTMy (zerknijcie na ostatnie slajdy z wprowdzenia do RNN (laboratoria 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
